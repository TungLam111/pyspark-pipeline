{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3346cd4",
   "metadata": {},
   "source": [
    "Dear teachers, i was doing the lab 04 assignment and successfully find itemsets of size 2 with count = 8831 (itemsets.txt and support = 100). I was\n",
    "having problems with memory when working with size 3. Then I tried with bigger support value = 300 to see if it works better but the problem's still there. My slave\n",
    "machines died bc of full disk and I have to delete and re-setup all. I also put some works of finding confidences below please come to check. Thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6206c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c14b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66896a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d76258",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"yarn\").setAppName(\"Lab04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b13590e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate (conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e9a0952",
   "metadata": {},
   "outputs": [],
   "source": [
    "isDebug = True\n",
    "support = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6831a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isDebug:\n",
    "    sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec40e42",
   "metadata": {},
   "source": [
    "Some pre-processsing steps. From initial RDD, I map each line to a list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56f204e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lineRDD = (sc.textFile(\"hdfs://master:9000/data/itemsets.txt\", 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bdc9f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lineRDD = lineRDD.map(lambda line: line.strip().split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a800562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_integer(line):\n",
    "    line = list(set(line))\n",
    "    new_line = []\n",
    "    for i in range(len(line)):\n",
    "        new_line.append(int(line[i]))\n",
    "    return new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "baa578b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lineRDD = lineRDD.map(lambda line: convert_to_integer(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34fbc28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "countLineRDD = lineRDD.flatMap(lambda x: x).map(lambda x: (x, 1)).reduceByKey(lambda a , b: a + b).filter(lambda x: x[1] >= support )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba320cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "supportRdd = countLineRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f471f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Flatten a list of list and integer\n",
    "'''\n",
    "def flatten_list(tuple_line):\n",
    "    new_list_line = list(tuple_line[0])\n",
    "    new_list_line.append(tuple_line[1])\n",
    "    return tuple(new_list_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6bde7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Flatten a list of list\n",
    "'''\n",
    "def flatten_list_of_list(tuple_line):\n",
    "    new_list_line = list(tuple_line[0])\n",
    "    for i in tuple_line[1]:\n",
    "        new_list_line.append(i)\n",
    "    return tuple(new_list_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7299b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute corpus support list for each line in RDD (each line represents for a list of integers.\n",
    "In order to make it, I use itertools.product function with some additional logics to \n",
    "find correct corpus set\n",
    "For example:\n",
    "[1,2,3,4] phase 2 -> [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)]\n",
    "[1,2,3,4] phase 3 -> [(1,2,3), (1,2,4), (2,3,4), (1,3,4)]\n",
    "\n",
    "  - To reuse data, corpus of phase n + 1 can be calculated from corpus of phase n\n",
    " '''\n",
    "def computeCorpusSupportList(previous_list, base_list , phase):\n",
    "    cartesian_list = list(itertools.product(previous_list, base_list))\n",
    "    if phase > 2:\n",
    "        cartesian_list = [flatten_list(item) for item in cartesian_list]\n",
    "    cartesian_list = [(tuple(sorted(set(item))), 1) for item in cartesian_list] \n",
    "    cartesian_hash = {}\n",
    "    for (a, b) in cartesian_list:\n",
    "        if cartesian_hash.get(a) == None:\n",
    "            cartesian_hash[a] = 0 \n",
    "            \n",
    "        cartesian_hash[a] += b\n",
    "    \n",
    "    cartesian_list = [ key for key in cartesian_hash if cartesian_hash[key] >= phase and len(key) == phase]\n",
    "    return cartesian_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b5a0e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Find candidate support list. After each phase of counting support list, we want to propose the next support\n",
    "list based on previous one\n",
    "To make it, I use .cartesian() function of RDD that is similar to itertools.product() but used for RDD\n",
    "'''\n",
    "def computeCandidateSupportRDD(rdd, phase):\n",
    "    cartesianRdd = rdd.map(lambda x: x[0])\n",
    "    cartesianRdd = cartesianRdd.cartesian(cartesianRdd)\n",
    "    if phase > 2 :\n",
    "        cartesianRdd = cartesianRdd.map(lambda x: flatten_list_of_list(x))\n",
    "    cartesianRdd = cartesianRdd.map(lambda x: (tuple(sorted(set(x))), 1))\n",
    "    cartesianRdd = cartesianRdd.reduceByKey(lambda a , b: a+b)\n",
    "    cartesianRdd = cartesianRdd.filter(lambda x: x[1] >= math.factorial(phase) and len(x[0]) == phase)\n",
    "    cartesianRdd = cartesianRdd.map(lambda x: (x[0], 1))\n",
    "    return cartesianRdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5a427ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_phase = 1\n",
    "corpus_lineRdd = lineRDD.map(lambda line: (line, line))\n",
    "union_rdd = supportRdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5747d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_phase = current_phase + 1\n",
    "    \n",
    "# print(\"current phase = \", current_phase)\n",
    "    \n",
    "#     # find candidate from previous supportRdd \n",
    "# candidate_supportRdd = computeCandidateSupportRDD(supportRdd, current_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5990d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate_supportRdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c05210cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_lineRdd = corpus_lineRdd.map(lambda line: (computeCorpusSupportList(line[0] , line[1], current_phase) , line[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f90de6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # apply mapReduce for the corpus\n",
    "# corpus_count_rdd = corpus_lineRdd.flatMap(lambda x: x[0]).map(lambda x: (x, 1))\n",
    "# corpus_count_rdd = corpus_count_rdd.reduceByKey(lambda a, b : a + b).filter(lambda z : z[1] >= support)\n",
    "        \n",
    "# # find intersection\n",
    "# supportRdd = candidate_supportRdd.join(corpus_count_rdd).map(lambda x: (x[0], x[1][1]))\n",
    "    \n",
    "# union_rdd.union(supportRdd)\n",
    "# print(supportRdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f97f2f03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current phase =  2\n",
      "Done candidate_supportRdd\n",
      "Done corpus_lineRdd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 138:===================================================> (234 + 2) / 240]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Find frequent itemsets of size 2.\n",
    "The idea is similar to standard A_prior but instead of check if each item in candidate sets is available in every order and\n",
    "then apply map-reduce method, I apply map-reduce for corpus first, and then find intersection later. \n",
    "'''\n",
    "while True:\n",
    "    \n",
    "    print(\"current phase = \", 2)\n",
    "    \n",
    "    # find candidate from previous supportRdd \n",
    "    candidate_supportRdd = computeCandidateSupportRDD(supportRdd, 2)\n",
    "    \n",
    "    print(\"Done candidate_supportRdd\")\n",
    "    \n",
    "    # find corpus for each line. through out the time, corpus_lineRdd is updated but still keep base \"line[1]\"\n",
    "    corpus_lineRdd = corpus_lineRdd.map(lambda line: (computeCorpusSupportList(line[0] , line[1], 2) , line[1]))    \n",
    "    \n",
    "    print(\"Done corpus_lineRdd\")\n",
    "    \n",
    "    # apply mapReduce for the corpus\n",
    "    corpus_count_rdd = corpus_lineRdd.flatMap(lambda x: x[0]).map(lambda x: (x, 1))\n",
    "    corpus_count_rdd = corpus_count_rdd.reduceByKey(lambda a, b : a + b).filter(lambda z : z[1] >= support)\n",
    "        \n",
    "    # inner join 2 rdd to get intersections \n",
    "    supportRdd = candidate_supportRdd.join(corpus_count_rdd).map(lambda x: (x[0], x[1][1]))\n",
    "    \n",
    "    union_rdd = union_rdd.union(supportRdd)\n",
    "    print(supportRdd.count())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd0ef1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNumber of requent itemsets of size 2 which has minSupport 100 is 8831\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Number of requent itemsets of size 2 which has minSupport 100 is 8831\n",
    "Number of requent itemsets of size 2 which has minSupport 300 is 1816\n",
    "\n",
    "To find all frequent itemsets, just run the while loop until supportRdd is empty. Due to large computation,\n",
    "I would like to find sets of size 2 and 3. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b87c0070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = sc.parallelize([(1,10), (2,20), (3, 30), (4,40)])\n",
    "# B = sc.parallelize([(9, 1), (2, 1), (3, 1)])\n",
    "# B.join(A).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4395808",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current phase =  3\n",
      "Done candidate_supportRdd\n",
      "Done corpus_lineRdd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 144:======================================>          (45590 + 2) / 57600]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/09 09:57:51 WARN HeartbeatReceiver: Removing executor 2 with no recent heartbeats: 160309 ms exceeds timeout 120000 ms\n",
      "22/12/09 09:57:51 WARN HeartbeatReceiver: Removing executor 1 with no recent heartbeats: 134399 ms exceeds timeout 120000 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 144:======================================>          (45590 + 2) / 57600]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/09 09:59:52 ERROR Utils: Uncaught exception in thread kill-executor-thread\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.killExecutors(CoarseGrainedSchedulerBackend.scala:890)\n",
      "\tat org.apache.spark.SparkContext.killAndReplaceExecutor(SparkContext.scala:1825)\n",
      "\tat org.apache.spark.HeartbeatReceiver$$anon$2.$anonfun$run$2(HeartbeatReceiver.scala:214)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.HeartbeatReceiver$$anon$2.run(HeartbeatReceiver.scala:211)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 10 more\n",
      "22/12/09 09:59:52 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending KillExecutors(List(2)) to AM was unsuccessful\n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from /192.1.1.30:44570 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\n",
      "\tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from /192.1.1.30:44570 in 120 seconds\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)\n",
      "\t... 7 more\n",
      "22/12/09 09:59:52 WARN NettyRpcEnv: Ignored failure: org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from /192.1.1.30:44570 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 144:======================================>          (45590 + 2) / 57600]\r"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For size = 3, some values can be reused to save computing resources. \n",
    "'''\n",
    "while True:    \n",
    "    print(\"current phase = \", 3)\n",
    "    \n",
    "    # find candidate from previous supportRdd \n",
    "    candidate_supportRdd = computeCandidateSupportRDD(supportRdd, 3)\n",
    "    \n",
    "    print(\"Done candidate_supportRdd\")\n",
    "    \n",
    "    # find corpus for each line. through out the time, corpus_lineRdd is updated but still keep base \"line[1]\"\n",
    "    corpus_lineRdd = corpus_lineRdd.map(lambda line: (computeCorpusSupportList(line[0] , line[1], 3) , line[1]))    \n",
    "    \n",
    "    print(\"Done corpus_lineRdd\")\n",
    "    \n",
    "    # apply mapReduce for the corpus\n",
    "    corpus_count_rdd = corpus_lineRdd.flatMap(lambda x: x[0]).map(lambda x: (x, 1))\n",
    "    corpus_count_rdd = corpus_count_rdd.reduceByKey(lambda a, b : a + b).filter(lambda z : z[1] >= support)\n",
    "        \n",
    "    # find intersection\n",
    "    supportRdd = candidate_supportRdd.join(corpus_count_rdd).map(lambda x: (x[0], x[1][1]))\n",
    "    \n",
    "    union_rdd = union_rdd.union(supportRdd)\n",
    "    print(supportRdd.count())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5adb106f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNumber of requent itemsets of size 3 which has minSupport 100 is \\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Number of frequent itemsets of size 3 which has minSupport 100 is\n",
    "Number of frequent itemsets of size 3 which has minSupport 300 is \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e999600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2346"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "To use result of problem 1 for problem 2, each time when finding out the new support sets, I unify it to union_rdd\n",
    "in the format (A, k) with A is itemset (can be single value or a list), k is support of A in dataset\n",
    "'''\n",
    "union_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b3b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filter out itemsets of size 2 and 3.\n",
    "'''\n",
    "freq_item_set_tripleton = union_rdd.filter(lambda x: isinstance(x[0], tuple) and len(x[0]) == 3)\n",
    "freq_item_set_doubleton = union_rdd.filter(lambda x: isinstance(x[0], tuple) and len(x[0]) == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a2b9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq_item_set_tripleton.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33cd099",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_item_set_doubleton.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ac7084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The idea is that, to find confidence for each association rules (X,Y) -> Z, we have to compute support for (X,Y)\n",
    "which is available in doubleton sets. Because of that, I want to find a way to make (X,Y) as keys and use join() operation.\n",
    "'''\n",
    "rdd1 = freq_item_set_tripleton.flatMap(lambda line: [(((line[0][0], line[0][1]), line[0][2]), line[1]), (((line[0][1], line[0][2]), line[0][0]), line[1]), (((line[0][0], line[0][2]), line[0][1]) ,line[1])]).map(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5caca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = rdd1.map(lambda x: (x[0][0] , x))\n",
    "rdd_join = rdd1.join(freq_item_set_doubleton) # Now we can find confidence values by some divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238f637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
